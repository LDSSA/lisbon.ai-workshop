{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Unit 6 - Hyper-parameters - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost done, keep up your courage! I hear there is beer after this. Anyway. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that we keep printing `cross val scores` for each set of hyper parameters we try. This has probably risen a certain twitch in the engineering minded among us: _\"Why am I clicking away at things? I can search this space programatically!\"_. \n",
    "\n",
    "Yes. Yes you can. Let's go do that then. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from utils import load_data, visualizations\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's start with our ``Ying-Yang```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data.get_ying_yang(n_points=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's fit a tree to it. Let's choose some parameters without thinkign too much about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = DecisionTreeClassifier(max_depth=2, min_samples_split=6)\n",
    "visualizations.plot_data(model=t, \n",
    "                         data=data, \n",
    "                         feature1='a', \n",
    "                         feature2='b', \n",
    "                         target='c', \n",
    "                         out_of_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we didn't do a particularly great job. We can definitely do better than this. \n",
    "\n",
    "Let's create a parameter grid of ``max_depth``, and ``min_samples_split``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [1, 2, 4, 8, 16, 32, 64], \n",
    "    'min_samples_split': [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can [Grid Search](scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) it. There are definitely smarter ways to do this (following gradients, etc etc), but let's go for the more intuitive version: brute force search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=t,              # <-- this object will behave as if it were a classifier \n",
    "                  param_grid=param_grid)\n",
    "\n",
    "gs.fit(data[['a', 'b']], data['c']);        # <-- here it runs on our grid of possible hyper params,\n",
    "                                            # and stores the cross val scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations.plot_hyper_parameters(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks as thought the best results are about the ``max_depth==8``  and ``min_samples_split==32``. \n",
    "\n",
    "Let's try that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = DecisionTreeClassifier(max_depth=8, min_samples_split=32)\n",
    "visualizations.plot_data(model=t, \n",
    "                         data=data, \n",
    "                         feature1='a', \n",
    "                         feature2='b', \n",
    "                         target='c', \n",
    "                         out_of_sample=True, \n",
    "                         probabilities=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to underestimate the importance of observing our results. After all, couldn't we just programatically use the best estimator based on a certain metric? While that is possible, generally the combination of human \"this doesn't look right\" type intuition with the hard optimization metrics works betterÂ than each of the parts. \n",
    "\n",
    "And now, to our final exercise, and then beers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
